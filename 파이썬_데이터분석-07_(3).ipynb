{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNl0VljtpnHHC2tjKq7Jvh5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### 2024년 1학기 Python 기초입문 - v 1.0"],"metadata":{"id":"QrhWEvhxmR9T"}},{"cell_type":"markdown","source":["\n","[colaboratory로 실행](https://colab.research.google.com/drive/1S9LouE7q5D1xe_qkcm_UuWj6rnctEJ4Q?usp=sharing)\n","\n","# 파이썬 데이터 분석"],"metadata":{"id":"r7UjGHUHrnW7"}},{"cell_type":"markdown","source":["### pytorch LSTM 예제\n","#### 순환 신경망(Recurrent Neural Network, RNN)\n","  - 입력과 출력을 시퀀스 단위로 처리하는 시퀀스(Sequence) 모델\n","  - 언어 모델과 같이 순서가 있는 데이터에 사용하는 딥러닝 구조\n","  \n"," <img src= 'https://miro.medium.com/v2/resize:fit:1194/1*B0q2ZLsUUw31eEImeVf3PQ.png' align=\"center\" height=200 widrh=300>\n","\n"," - 시계열에서는 자기상관관계의 크기를 신경망을 가중치를 통해서 학습함.\n"," - 선형모형에 대해서 비선형적인 모형의 대표가 되고 있음.\n","\n","\n","#### Tensorflowr tutorial\n","  - [텐서플로우 구조화된 데이터 예제](https://www.tensorflow.org/tutorials/structured_data/time_series?hl=ko)\n","\n","  - 전체적인 모델 연구과정을 잘 보여줌\n","  - 데이터에 대한 전처리 및 전환\n","  - 모델의 설계와 훈련 과정\n","  - 벤치마킹 과정 등을 잘 보여주는 예제\n","  \n","\n","\n"],"metadata":{"id":"3XNBInqKGEtM"}},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","import os\n","import pandas as pd\n","import numpy as np\n","import requests                 #네트워크 접근 라이브러리\n","from bs4 import BeautifulSoup   #웹사이트 접근하는 라이브러리\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","target = '삼성전자'\n","\n","# 종목 이름을 입력하면 종목에 해당하는 코드를 불러와\n","# 네이버 금융(http://finance.naver.com)에 넣어줌\n","\n","code_krx = pd.read_html('http://kind.krx.co.kr/corpgeneral/corpList.do?method=download&searchType=13', header=0)[0]\n","# 종목코드가 6자리이기 때문에 6자리를 맞춰주기 위해 설정해줌\n","code_krx.종목코드 = code_krx.종목코드.map('{:06d}'.format)\n","# 우리가 필요한 것은 회사명과 종목코드이기 때문에 필요없는 column들은 제외해준다.\n","code_krx = code_krx[['회사명', '종목코드']]\n","# code_krx\n","target_code = code_krx[code_krx['회사명']==target]\n","target_code"],"metadata":{"id":"Q1fyokvdGI6y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 종목코드로 네이버에서 종목 주소 생성 확인\n","def get_url(code): #(item_name, code_df):\n","    url = 'https://finance.naver.com/item/sise_day.naver?code='+'{code}'.format(code=code).lstrip()\n","    return url\n","code = target_code['종목코드'].iloc[0]\n","url = get_url(code)\n","url"],"metadata":{"id":"gJIVEsmUGPP-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 정규분포\n","# 연속 continuous 을 표현하기 위해서 x 관측의 범위를 0 에서, 30 까지로 하고 표본은 1000\n","x = np.linspace(0, 30, 1000)\n","print(f\"sample's mean : {np.mean(x)}\")\n","print(f\"sample's median : {np.median(x)}\") # 정규분포이므로 좌우 대칭. 아닌경우 한쪽으로 기우어진 분포\n"],"metadata":{"id":"7CgX6KlMGQKm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 위에서 찾은 네이버 타겟종목 페이지에서 마지막 페이지 크기 확인\n","url_page = url + '&page=1'\n","headers = {'User-agent':'Mozilla/5.0'}\n","html = requests.get(url_page, headers=headers).text\n","soup = BeautifulSoup(html,\"html.parser\")\n","tags = soup.find_all('a')\n","# print(tags[11][\"href\"])\n","last_page = tags[11][\"href\"]\n","last_page = last_page.split('=')[2]\n","last_page\n","# print(f\"target_code :회사명 == [{target_code['회사명'].iloc[0]}] 종목코드 == [{target_code['종목코드'].iloc[0]}] 마지막 페이지 == [{last_page}]\")"],"metadata":{"id":"xKiaWejGGSZO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 한글로 된 컬럼명을 영어로 바꿔줌\n","stock_data = df_code.rename(columns= {'날짜': 'date', '종가': 'close', '전일비': 'diff', '시가': 'open', '고가': 'high', '저가': 'low', '거래량': 'volume'})\n","# 데이터의 타입을 int형으로 바꿔줌\n","stock_data[['close', 'diff', 'open', 'high', 'low', 'volume']] = stock_data[['close', 'diff', 'open', 'high', 'low', 'volume']].astype(float)\n","# 컬럼명 'date'의 타입을 date로 바꿔줌\n","stock_data['date'] = pd.to_datetime(stock_data['date'])\n","# 일자(date)를 기준으로  정렬\n","stock_data = stock_data.sort_values(by=['date'],ascending = True)\n","# 상위 5개 데이터 확인\n","print(stock_data.shape)\n","stock_data.reset_index(drop=True, inplace=True)\n","stock_data.head(15)"],"metadata":{"id":"TKGQJzZZGTEc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["slice_data = stock_data.iloc[-300:,:] # 특정 기간을 어림으로 분리\n","slice_data.reset_index(drop=True, inplace=True)\n","plt.figure(figsize=(20,5))\n","plt.plot(slice_data['date'],slice_data['close'])\n","plt.show\n","\n","slice_data = slice_data.set_index('date')\n","slice_data.to_csv('stock_data_01.csv')"],"metadata":{"id":"eylCCfaWGUgc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 데이터 로드 및 정규화"],"metadata":{"id":"tVO0s_faJpdS"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import random\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# 데이터 불러오기\n","df = pd.read_csv('./stock_data_01.csv')\n","\n","# 입력으로 7일 데이터로 (window size) 들어가고 batch size는 임의로 지정\n","seq_length = 5\n","batch = 10\n","\n","# 데이터를 역순으로 정렬하여 전체 데이터의 70% 학습, 30% 테스트에 사용\n","# df = df[::-1] # 데이터 저장 형태에 따라 수정\n","train_size = int(len(df)*0.95) # 일반적으로는 8:1:1\n","train_set = df[0:train_size]\n","test_set = df[train_size-seq_length:]\n","#본 예제에는 validation 구간이 없슴. 사실 적정한 학습을 위해서는 validataion dataset 구간이 필요함."],"metadata":{"id":"ADm8SnUDJqhC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_set = train_set.set_index('date')\n","print(f\" train_dataset : \\n {train_set.head()}\")\n","test_set = test_set.set_index('date')\n","print(f\" \\n test_dataset : \\n {test_set.head()}\")"],"metadata":{"id":"0xv4tlWOJska"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Input scale\n","scaler_x = MinMaxScaler()\n","scaler_x.fit(train_set.iloc[:, :-1])\n","\n","train_set.iloc[:, :-1] = scaler_x.transform(train_set.iloc[:, :-1])\n","test_set.iloc[:, :-1] = scaler_x.transform(test_set.iloc[:, :-1])\n","\n","# Output scale\n","scaler_y = MinMaxScaler()\n","scaler_y.fit(train_set.iloc[:, [-1]])\n","\n","train_set.iloc[:, -1] = scaler_y.transform(train_set.iloc[:, [-1]])\n","test_set.iloc[:, -1] = scaler_y.transform(test_set.iloc[:, [-1]])"],"metadata":{"id":"fA-JnPUqJuHK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch import optim\n","\n","from torch.utils.data import TensorDataset # 텐서데이터셋\n","from torch.utils.data import DataLoader # 데이터로더"],"metadata":{"id":"mb-U0DNYJvsK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# GPU\n","use_gpu = False             # 'use gpu'\n","gpu = 0                    # 'gpu'\n","use_multi_gpu = False      # 'use multiple gpus'\n","devices = '0,1,2,3'        # 'device ids of multile gpus'\n","\n","# use_gpu = True if torch.cuda.is_available() and use_gpu else False\n","# if use_gpu and use_multi_gpu:\n","#     dvices = devices.replace(' ', '')\n","#     device_ids = devices.split(',')\n","#     device_ids = [int(id_) for id_ in device_ids]\n","#     gpu = device_ids[0]\n","\n","if use_gpu:\n","    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu) if not use_multi_gpu else devices\n","    device = torch.device('cuda:{}'.format(gpu))\n","    print('Use GPU: cuda:{}'.format(gpu))\n","else:\n","    device = torch.device('cpu')\n","    print('Use CPU')\n","\n","device"],"metadata":{"id":"YMeD_V1yJxPV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터셋 생성 함수\n","def build_dataset(time_series, seq_length):\n","    dataX = []\n","    dataY = []\n","    for i in range(0, len(time_series)-seq_length):\n","        _x = time_series[i:i+seq_length, :]\n","        _y = time_series[i+seq_length, [-1]]\n","        # print(_x, \"-->\",_y)\n","        dataX.append(_x)\n","        dataY.append(_y)\n","\n","    return np.array(dataX), np.array(dataY)\n","\n","trainX, trainY = build_dataset(np.array(train_set), seq_length)\n","testX, testY = build_dataset(np.array(test_set), seq_length)\n","\n","# 텐서로 변환\n","trainX_tensor = torch.FloatTensor(trainX)\n","trainY_tensor = torch.FloatTensor(trainY)\n","\n","testX_tensor = torch.FloatTensor(testX)\n","testY_tensor = torch.FloatTensor(testY)\n","\n","# 텐서 형태로 데이터 정의\n","dataset = TensorDataset(trainX_tensor, trainY_tensor)\n","\n","# 데이터로더는 기본적으로 2개의 인자를 입력받으며 배치크기는 통상적으로 2의 배수를 사용\n","dataloader = DataLoader(dataset,\n","                        batch_size=batch,\n","                        shuffle=True,\n","                        drop_last=True)"],"metadata":{"id":"0SUrzU5_Jy-y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 설정값\n","data_dim = 5\n","hidden_dim = 10\n","output_dim = 1\n","learning_rate = 0.01\n","nb_epochs = 100\n","\n","class Net(nn.Module):\n","    # # 기본변수, layer를 초기화해주는 생성자\n","    def __init__(self, input_dim, hidden_dim, seq_len, output_dim, layers):\n","        super(Net, self).__init__()\n","        self.hidden_dim = hidden_dim\n","        self.seq_len = seq_len\n","        self.output_dim = output_dim\n","        self.layers = layers\n","\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=layers,\n","                            # dropout = 0.1,\n","                            batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim, bias = True)\n","\n","    # 학습 초기화를 위한 함수\n","    def reset_hidden_state(self):\n","        self.hidden = (\n","                torch.zeros(self.layers, self.seq_len, self.hidden_dim),\n","                torch.zeros(self.layers, self.seq_len, self.hidden_dim))\n","\n","    # 예측을 위한 함수\n","    def forward(self, x):\n","        x, _status = self.lstm(x)\n","        x = self.fc(x[:, -1])\n","        return x"],"metadata":{"id":"IoDLCtc1J1nK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_model(model, train_df, num_epochs = None, lr = None, verbose = 10, patience = 10):\n","\n","    criterion = nn.MSELoss().to(device)\n","    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n","    nb_epochs = num_epochs\n","\n","    # epoch마다 loss 저장\n","    train_hist = np.zeros(nb_epochs)\n","\n","    for epoch in range(nb_epochs):\n","        avg_cost = 0\n","        total_batch = len(train_df)\n","\n","        for batch_idx, samples in enumerate(train_df):\n","\n","            x_train, y_train = samples\n","\n","            # seq별 hidden state reset\n","            model.reset_hidden_state()\n","\n","            # H(x) 계산\n","            outputs = model(x_train)\n","\n","            # cost 계산\n","            loss = criterion(outputs, y_train)\n","\n","            # cost로 H(x) 개선\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            avg_cost += loss/total_batch\n","\n","        train_hist[epoch] = avg_cost\n","\n","        if epoch % verbose == 0:\n","            print('Epoch:', '%04d' % (epoch), 'train loss :', '{:.4f}'.format(avg_cost))\n","\n","        # patience번째 마다 early stopping 여부 확인\n","        if (epoch % patience == 0) & (epoch != 0):\n","\n","            # loss가 커졌다면 early stop\n","            if train_hist[epoch-patience] < train_hist[epoch]:\n","                print('\\n Early Stopping')\n","\n","                break\n","\n","    return model.eval(), train_hist"],"metadata":{"id":"3OBEapaUJ2_k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 학습\n","net = Net(data_dim, hidden_dim, seq_length, output_dim, 1).to(device)\n","model, train_hist = train_model(net, dataloader, num_epochs = nb_epochs, lr = learning_rate, verbose = 20, patience = 10)"],"metadata":{"id":"2aG9W3kkJ5vU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# epoch별 손실값\n","fig = plt.figure(figsize=(10, 4))\n","plt.plot(train_hist, label=\"Training loss\")\n","plt.legend()\n","plt.show()"],"metadata":{"id":"renZJJv7J6ov"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 저장\n","PATH = \"./Timeseries_LSTM_data-02-stock_daily_.pth\"\n","torch.save(model.state_dict(), PATH)\n","\n","# 불러오기\n","model = Net(data_dim, hidden_dim, seq_length, output_dim, 1).to(device)\n","model.load_state_dict(torch.load(PATH), strict=False)\n","model.eval()"],"metadata":{"id":"7CLbdo-fJ77a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 예측 테스트\n","with torch.no_grad():\n","    pred = []\n","    for pr in range(len(testX_tensor)):\n","\n","        model.reset_hidden_state()\n","\n","        predicted = model(torch.unsqueeze(testX_tensor[pr], 0))\n","        predicted = torch.flatten(predicted).item()\n","        pred.append(predicted)\n","\n","    # INVERSE\n","    pred_inverse = scaler_y.inverse_transform(np.array(pred).reshape(-1, 1))\n","    testY_inverse = scaler_y.inverse_transform(testY_tensor)\n","\n","def MAE(true, pred):\n","    return np.mean(np.abs(true-pred))\n","\n","print('MAE SCORE : ', MAE(pred_inverse, testY_inverse))"],"metadata":{"id":"adUWp6WnJ-ak"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig = plt.figure(figsize=(8,3))\n","plt.plot(np.arange(len(pred_inverse)), pred_inverse, label = 'pred')\n","plt.plot(np.arange(len(testY_inverse)), testY_inverse, label = 'true')\n","plt.title(\"Loss plot\")\n","plt.show()"],"metadata":{"id":"j2XkRHNaJ_K4"},"execution_count":null,"outputs":[]}]}